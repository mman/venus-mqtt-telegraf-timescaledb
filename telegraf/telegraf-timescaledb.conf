# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "5s"

  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"

  ## Collected metrics are rounded to the precision specified. Precision is
  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  ##
  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  precision = "0s"

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = true

# STEP 0: Read metrics from MQTT topic(s)
[[inputs.mqtt_consumer]]
  servers = ["tcp://${VENUS_HOST}:1883"]
  topics = [
    "N/#"
  ]

  data_format = "json_v2"

  [[inputs.mqtt_consumer.json_v2]]
    [[inputs.mqtt_consumer.json_v2.field]]
      path = "value"

# STEP 1: drop measurements without value, rename string typed value
[[processors.starlark]]
  order = 1

  source = '''
def apply(metric):
  # drop measurements without value
  value = metric.fields.get("value")
  if value == None:
    return None
  # drop measurements with 'object' type value
  if type(value) == "object":
    return None
  # rename measurements with 'string' type to valueString
  if type(value) == "string":
    metric.fields["valueString"] = value
    metric.fields.pop("value")
  else:
    # rename other types to floatValue
    metric.fields["valueFloat"] = float(value)
    metric.fields.pop("value")
  return metric
'''

# STEP 2: extract appropriate tags
[[processors.regex]]
  order = 2

  # extract `portalId` from mqtt topic
  [[processors.regex.tags]]
    key = "topic"
    pattern = '^N\/([a-zA-Z0-9]+)\/(.*)$'
    replacement = "${1}"
    result_key = "portalId"

  # extract `instance` from mqtt topic
  [[processors.regex.tags]]
    key = "topic"
    pattern = '^N\/([a-zA-Z0-9]+)\/([a-zA-Z0-9]+)\/([0-9]+)\/(.*)$'
    replacement = "${3}"
    result_key = "instance"

  # drop `N`, `portalId`, `component`, and `instance` from mqtt topic
  # modify topic to include fully qualified path
  [[processors.regex.tags]]
    key = "topic"
    pattern = '^N\/([a-zA-Z0-9]+)\/([a-zA-Z0-9]+)\/([0-9]+)\/(.*)$'
    replacement = "${2}/${4}"

# STEP 3: rename measurement from default mqtt_consumer to parsed topic
[[processors.converter]]
  order = 3

  [processors.converter.tags]
    measurement = ["topic"]

# STEP 4: convert measurement name to code
[[processors.lookup]]
  order = 4

  format = "json"
  files = ["/etc/telegraf/venus-mqtt-topic-to-code-lut.json"]
  key = 'com.victronenergy.{{ .Name }}'

# STEP 5: drop measurements without code, rename measurement
[[processors.starlark]]
  order = 5

  source = '''
def apply(metric):
  # drop metrics without code
  if metric.tags.get("code") == None:
    return None
  # drop unneccessary whenToLog tag
  metric.tags.pop("whenToLog")
  topic = metric.name
  metric.name = "measurement"
  metric.tags.update([("topic", topic)])
  return metric
'''

[[outputs.file]]
# files = ["stdout"]
#data_format = "json"
# #json_timestamp_units = "1s"
data_format = "influx"
influx_sort_fields = true

# Publishes metrics to a postgresql database
[[outputs.postgresql]]
connection = "postgres://venus:s3cr4t@host.docker.internal/venus?sslmode=disable"
schema = "venus"

create_templates = [
    '''CREATE TABLE {{ .table }} ({{ .columns }})''',
    '''SELECT create_hypertable({{ .table|quoteLiteral }}, 'time', chunk_time_interval => INTERVAL '1 hour')'''
]

#   ## Store tags as foreign keys in the metrics table. Default is false.
#   # tags_as_foreign_keys = false
#
#   ## Suffix to append to table name (measurement name) for the foreign tag table.
#   # tag_table_suffix = "_tag"
#
#   ## Deny inserting metrics if the foreign tag can't be inserted.
#   # foreign_tag_constraint = false
#
#   ## Store all tags as a JSONB object in a single 'tags' column.
#   # tags_as_jsonb = false
#
#   ## Store all fields as a JSONB object in a single 'fields' column.
#   # fields_as_jsonb = false
#
#   ## Templated statements to execute when creating a new table.
#   # create_templates = [
#   #   '''CREATE TABLE {{ .table }} ({{ .columns }})''',
#   # ]
#
#   ## Templated statements to execute when adding columns to a table.
#   ## Set to an empty list to disable. Points containing tags for which there is no column will be skipped. Points
#   ## containing fields for which there is no column will have the field omitted.
#   # add_column_templates = [
#   #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join ", ADD COLUMN IF NOT EXISTS " }}''',
#   # ]
#
#   ## Templated statements to execute when creating a new tag table.
#   # tag_table_create_templates = [
#   #   '''CREATE TABLE {{ .table }} ({{ .columns }}, PRIMARY KEY (tag_id))''',
#   # ]
#
#   ## Templated statements to execute when adding columns to a tag table.
#   ## Set to an empty list to disable. Points containing tags for which there is no column will be skipped.
#   # tag_table_add_column_templates = [
#   #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join ", ADD COLUMN IF NOT EXISTS " }}''',
#   # ]
#
#   ## The postgres data type to use for storing unsigned 64-bit integer values (Postgres does not have a native
#   ## unsigned 64-bit integer type).
#   ## The value can be one of:
#   ##   numeric - Uses the PostgreSQL "numeric" data type.
#   ##   uint8 - Requires pguint extension (https://github.com/petere/pguint)
#   # uint64_type = "numeric"
#
#   ## When using pool_max_conns>1, and a temporary error occurs, the query is retried with an incremental backoff. This
#   ## controls the maximum backoff duration.
#   # retry_max_backoff = "15s"
#
#   ## Approximate number of tag IDs to store in in-memory cache (when using tags_as_foreign_keys).
#   ## This is an optimization to skip inserting known tag IDs.
#   ## Each entry consumes approximately 34 bytes of memory.
#   # tag_cache_size = 100000
#
#   ## Enable & set the log level for the Postgres driver.
#   # log_level = "warn" # trace, debug, info, warn, error, none
